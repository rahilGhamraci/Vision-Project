{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolo-Weights/yolov5n.pt' with new 'model=yolo-Weights/yolov5nu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5nu.pt to 'yolo-Weights\\yolov5nu.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.31M/5.31M [00:06<00:00, 887kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 5111.1ms\n",
      "Speed: 99.2ms preprocess, 5111.1ms inference, 53.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2721.3ms\n",
      "Speed: 72.4ms preprocess, 2721.3ms inference, 28.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3268.7ms\n",
      "Speed: 18.4ms preprocess, 3268.7ms inference, 12.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2817.5ms\n",
      "Speed: 43.4ms preprocess, 2817.5ms inference, 14.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3053.9ms\n",
      "Speed: 29.8ms preprocess, 3053.9ms inference, 27.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2727.8ms\n",
      "Speed: 20.3ms preprocess, 2727.8ms inference, 16.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2683.1ms\n",
      "Speed: 21.7ms preprocess, 2683.1ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2746.9ms\n",
      "Speed: 15.6ms preprocess, 2746.9ms inference, 23.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3391.3ms\n",
      "Speed: 18.1ms preprocess, 3391.3ms inference, 29.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2597.5ms\n",
      "Speed: 38.5ms preprocess, 2597.5ms inference, 31.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2850.4ms\n",
      "Speed: 45.5ms preprocess, 2850.4ms inference, 31.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2905.3ms\n",
      "Speed: 23.4ms preprocess, 2905.3ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3424.3ms\n",
      "Speed: 20.4ms preprocess, 3424.3ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3104.2ms\n",
      "Speed: 24.9ms preprocess, 3104.2ms inference, 20.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2789.6ms\n",
      "Speed: 36.1ms preprocess, 2789.6ms inference, 18.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2701.4ms\n",
      "Speed: 14.3ms preprocess, 2701.4ms inference, 22.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3773.0ms\n",
      "Speed: 47.0ms preprocess, 3773.0ms inference, 21.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2794.2ms\n",
      "Speed: 26.6ms preprocess, 2794.2ms inference, 31.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2852.4ms\n",
      "Speed: 18.4ms preprocess, 2852.4ms inference, 19.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3166.3ms\n",
      "Speed: 20.2ms preprocess, 3166.3ms inference, 30.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3181.4ms\n",
      "Speed: 14.2ms preprocess, 3181.4ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2943.4ms\n",
      "Speed: 13.1ms preprocess, 2943.4ms inference, 20.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2717.6ms\n",
      "Speed: 23.6ms preprocess, 2717.6ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2941.3ms\n",
      "Speed: 17.1ms preprocess, 2941.3ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2842.5ms\n",
      "Speed: 20.7ms preprocess, 2842.5ms inference, 30.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2985.0ms\n",
      "Speed: 28.1ms preprocess, 2985.0ms inference, 26.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3447.2ms\n",
      "Speed: 21.4ms preprocess, 3447.2ms inference, 20.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3695.7ms\n",
      "Speed: 35.3ms preprocess, 3695.7ms inference, 30.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2847.1ms\n",
      "Speed: 27.0ms preprocess, 2847.1ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2653.8ms\n",
      "Speed: 25.6ms preprocess, 2653.8ms inference, 29.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2876.6ms\n",
      "Speed: 21.1ms preprocess, 2876.6ms inference, 22.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2940.1ms\n",
      "Speed: 22.5ms preprocess, 2940.1ms inference, 30.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3323.4ms\n",
      "Speed: 21.4ms preprocess, 3323.4ms inference, 41.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 4195.3ms\n",
      "Speed: 18.3ms preprocess, 4195.3ms inference, 28.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2961.9ms\n",
      "Speed: 28.3ms preprocess, 2961.9ms inference, 28.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2939.0ms\n",
      "Speed: 47.6ms preprocess, 2939.0ms inference, 27.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3505.9ms\n",
      "Speed: 17.2ms preprocess, 3505.9ms inference, 23.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3000.1ms\n",
      "Speed: 47.9ms preprocess, 3000.1ms inference, 30.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2920.1ms\n",
      "Speed: 27.3ms preprocess, 2920.1ms inference, 28.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2766.5ms\n",
      "Speed: 30.9ms preprocess, 2766.5ms inference, 21.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2789.5ms\n",
      "Speed: 22.8ms preprocess, 2789.5ms inference, 20.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2710.7ms\n",
      "Speed: 20.4ms preprocess, 2710.7ms inference, 30.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2639.2ms\n",
      "Speed: 50.4ms preprocess, 2639.2ms inference, 31.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2627.6ms\n",
      "Speed: 25.8ms preprocess, 2627.6ms inference, 31.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2702.4ms\n",
      "Speed: 19.0ms preprocess, 2702.4ms inference, 22.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2556.5ms\n",
      "Speed: 22.9ms preprocess, 2556.5ms inference, 10.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2586.8ms\n",
      "Speed: 30.7ms preprocess, 2586.8ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2922.9ms\n",
      "Speed: 20.3ms preprocess, 2922.9ms inference, 26.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2606.0ms\n",
      "Speed: 16.9ms preprocess, 2606.0ms inference, 24.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2679.5ms\n",
      "Speed: 35.7ms preprocess, 2679.5ms inference, 18.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2610.5ms\n",
      "Speed: 20.9ms preprocess, 2610.5ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2584.6ms\n",
      "Speed: 26.1ms preprocess, 2584.6ms inference, 14.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2604.7ms\n",
      "Speed: 22.4ms preprocess, 2604.7ms inference, 20.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2539.4ms\n",
      "Speed: 41.0ms preprocess, 2539.4ms inference, 34.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2601.1ms\n",
      "Speed: 31.9ms preprocess, 2601.1ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2693.2ms\n",
      "Speed: 14.4ms preprocess, 2693.2ms inference, 30.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2687.7ms\n",
      "Speed: 22.6ms preprocess, 2687.7ms inference, 32.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3083.0ms\n",
      "Speed: 24.6ms preprocess, 3083.0ms inference, 23.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2682.1ms\n",
      "Speed: 15.6ms preprocess, 2682.1ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2631.3ms\n",
      "Speed: 20.7ms preprocess, 2631.3ms inference, 22.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2606.6ms\n",
      "Speed: 58.4ms preprocess, 2606.6ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 2879.6ms\n",
      "Speed: 19.2ms preprocess, 2879.6ms inference, 28.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2648.1ms\n",
      "Speed: 47.5ms preprocess, 2648.1ms inference, 31.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3787.0ms\n",
      "Speed: 90.5ms preprocess, 3787.0ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2867.3ms\n",
      "Speed: 12.9ms preprocess, 2867.3ms inference, 20.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2463.2ms\n",
      "Speed: 10.3ms preprocess, 2463.2ms inference, 20.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 2496.4ms\n",
      "Speed: 13.4ms preprocess, 2496.4ms inference, 24.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 2387.9ms\n",
      "Speed: 9.2ms preprocess, 2387.9ms inference, 21.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 2274.4ms\n",
      "Speed: 18.3ms preprocess, 2274.4ms inference, 20.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 2340.1ms\n",
      "Speed: 28.3ms preprocess, 2340.1ms inference, 37.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 1 cell phone, 2381.2ms\n",
      "Speed: 15.6ms preprocess, 2381.2ms inference, 30.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 1 cell phone, 2424.7ms\n",
      "Speed: 12.1ms preprocess, 2424.7ms inference, 30.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 1 cell phone, 2320.1ms\n",
      "Speed: 12.3ms preprocess, 2320.1ms inference, 30.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2438.1ms\n",
      "Speed: 9.2ms preprocess, 2438.1ms inference, 30.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 2 cell phones, 2452.7ms\n",
      "Speed: 19.5ms preprocess, 2452.7ms inference, 22.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 2427.4ms\n",
      "Speed: 15.6ms preprocess, 2427.4ms inference, 29.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2451.7ms\n",
      "Speed: 14.0ms preprocess, 2451.7ms inference, 20.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2476.5ms\n",
      "Speed: 7.6ms preprocess, 2476.5ms inference, 30.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 2722.4ms\n",
      "Speed: 10.1ms preprocess, 2722.4ms inference, 38.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2668.8ms\n",
      "Speed: 13.3ms preprocess, 2668.8ms inference, 26.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2650.3ms\n",
      "Speed: 10.0ms preprocess, 2650.3ms inference, 23.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 2619.2ms\n",
      "Speed: 15.0ms preprocess, 2619.2ms inference, 28.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2677.3ms\n",
      "Speed: 20.3ms preprocess, 2677.3ms inference, 23.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2871.3ms\n",
      "Speed: 31.8ms preprocess, 2871.3ms inference, 28.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 2914.4ms\n",
      "Speed: 14.0ms preprocess, 2914.4ms inference, 18.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 2707.2ms\n",
      "Speed: 12.3ms preprocess, 2707.2ms inference, 20.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 2424.0ms\n",
      "Speed: 14.5ms preprocess, 2424.0ms inference, 30.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2434.2ms\n",
      "Speed: 19.3ms preprocess, 2434.2ms inference, 22.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 2356.2ms\n",
      "Speed: 15.6ms preprocess, 2356.2ms inference, 26.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 2317.9ms\n",
      "Speed: 10.3ms preprocess, 2317.9ms inference, 28.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# libraries \n",
    "import cv2 # manipulate frames and video (display)\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort # to use the model of track \n",
    "from ultralytics import YOLO # to use yolo to track \n",
    "\n",
    "\n",
    "# Set environment variable to avoid duplicate library errors\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "\n",
    "# Initialize the DeepSort tracker (une instance )\n",
    "''' \n",
    "DeepSort uses :\n",
    "    Kalman Filters & Hungarian Algorithm for { association and prediction }\n",
    "    Neural network for { appearance-based re-identification }\n",
    "It ensures consistent tracking of objects \n",
    "    [even when they temporarily disappear from view or when the camera moves slightly]\n",
    "Each tracked object is assigned\n",
    "    a unique ID (to monitor that object)\n",
    "'''\n",
    "object_tracker = DeepSort()\n",
    "\n",
    "# Initialize the YOLO model (version 8) with a smaller model for faster inference\n",
    "# Use yolov5n for faster inference\n",
    "# DETAILS \n",
    "'''\n",
    "model = YOLO(weights, task=None, mode=None)\n",
    "weights (Required):\n",
    "    path to the custom model weights\n",
    "        Pretrained YOLOv8 weights:\n",
    "            \"yolov8'c'.pt\": c => can be ,n(nano) ,s(small) ,m(medium) ,l(large) ,x(extra large)\n",
    "task (Optional):\n",
    "    Specifies the task the model is meant to perform:\n",
    "        \"detect\": Object detection (default if not specified).\n",
    "        \"segment\": Instance segmentation.\n",
    "        \"classify\": Image classification.\n",
    "mode (Optional):\n",
    "    Specifies the mode of operation:\n",
    "        \"train\": Train a new or custom YOLO model.\n",
    "        \"val\": Validate the performance of the model on a dataset.\n",
    "        \"predict\": Make predictions on images or videos (default).\n",
    "        \"export\": Export the model for inference (e.g., ONNX, TensorRT, etc.).\n",
    "\n",
    "default : \n",
    "- model = YOLO(\"yolov8n-seg.pt/yolov8n.pt\")\n",
    "    Default task : \"segment\".\n",
    "    Default mode: \"predict\".\n",
    "- model = YOLO(\"yolov8n-cls.pt/yolov8n.pt\")\n",
    "    Default task : \"classify\".\n",
    "    Default mode: \"predict\".\n",
    "'''\n",
    "model = YOLO(\"yolo-Weights/yolov5n.pt\")  \n",
    "\n",
    "\n",
    "# code to start the webcam ( 0 => the actual default camera , in my case my laptop)\n",
    "# DETAILS \n",
    "'''\n",
    "cap = cv2.VideoCapture(source, apiPreference)\n",
    "READING of the video frame by frame \n",
    "Parameters:\n",
    "    source (required):\n",
    "        Specifies the video source.\n",
    "        Integer: Refers to the index of the camera.\n",
    "            0: Default camera \n",
    "            1: Second connected camera, and so on.\n",
    "        String: Path to a video file \n",
    "apiPreference (optional):\n",
    "Specifies which API backend to use (e.g., DirectShow, Media Foundation, etc.).\n",
    "Common values include:\n",
    "cv2.CAP_ANY (default): Auto-select the backend.\n",
    "cv2.CAP_DSHOW: DirectShow (Windows).\n",
    "cv2.CAP_AVFOUNDATION: AVFoundation (macOS).\n",
    "'''\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# these lines is to define the capture of cam width and the height \n",
    "# DETAILS \n",
    "'''\n",
    "cap.set(id number ,pixels values )\n",
    "- '3' => CAP_PROP_FRAME_WIDTH the video frame width in pixels.\n",
    "- '4' => CAP_PROP_FRAME_HEIGHT the video frame height in pixels.\n",
    "'''\n",
    "cap.set(3, 640)  \n",
    "cap.set(4, 480)  \n",
    "\n",
    "# Read class names from the model (reads all the classes that are available on the yolo model)\n",
    "# DETAILS\n",
    "'''\n",
    "COCO dataset\n",
    "model.names is a dictionnary : \n",
    "    {\n",
    "        (key: \"value\",)\n",
    "        0: \"person\",\n",
    "        1: \"bicycle\",\n",
    "        2: \"car\",\n",
    "        ...\n",
    "        79: \"toothbrush\"\n",
    "    }\n",
    "'''\n",
    "classNames = model.names  \n",
    "\n",
    "# just take the index of the object we want to detect , in our case , cell phone => 67\n",
    "phone_class_index = 67  \n",
    "\n",
    "# this line ensures that the web cam is indeed opened \n",
    "while cap.isOpened():\n",
    "    '''\n",
    "    cap.read() returns :\n",
    "        success: A boolean  frame was successfully read or not .\n",
    "        img: The actual frame (image) if success is true .\n",
    "    '''\n",
    "    success, img = cap.read()\n",
    "\n",
    "    '''cas frame not read sortir '''\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    # DETAILS\n",
    "    '''\n",
    "    img is the frame we read \n",
    "    the stream value \n",
    "        The stream=True means  'YOLO' will return results as a generator(with streaming)\n",
    "    means :\n",
    "        Instead of returning all detections at once in one frame it returns a set of results that way we can manipulate each as we want , it is also memory saving\n",
    "    '''\n",
    "    '''\n",
    "    here:\n",
    "        preprocessing \n",
    "            resizing the frame to treat it with yolo \n",
    "            normalisation ect \n",
    "        inference \n",
    "            runing cnn of yolo to detect \n",
    "        steaming \n",
    "            already explained \n",
    "    '''\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # Prepare a list to store detections for DeepSORT\n",
    "    detections = []\n",
    "\n",
    "    # Process results from YOLO\n",
    "    '''\n",
    "    for each results r we will have : \n",
    "        Bounding Box (r.boxes.xyxy):\n",
    "            (x1, y1, x2, y2).\n",
    "        Class Index (r.boxes.cls):\n",
    "            The index of the detected class \n",
    "        Confidence (r.boxes.conf):\n",
    "            The confidence score for the detection of the class (proba)\n",
    "    '''\n",
    "    # iterate all the results \n",
    "    for r in results:\n",
    "        # retrieve the box detection result of all objects \n",
    "        boxes = r.boxes\n",
    "        # this to only detect a phone \n",
    "        # iterate all the boxes \n",
    "        for box in boxes:\n",
    "            # retrieve the classe of detection \n",
    "            cls = int(box.cls[0])  \n",
    "\n",
    "            # If the detected class is 'cell phone'\n",
    "            '''do traitement , detection + tracking '''\n",
    "            if cls == phone_class_index:\n",
    "                '''get the coordinates'''\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                '''\n",
    "                    float to int\n",
    "                    because : \n",
    "                        Image pixels are discrete and indexed using integers.\n",
    "                '''\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "\n",
    "                # Add detection to list for DeepSORT (bbox, confidence, class)\n",
    "                detections.append(([x1, y1, x2, y2], box.conf[0], cls))\n",
    "\n",
    "                # DISPLAY \n",
    "                '''\n",
    "                display rectangle \n",
    "                cv2.rectangle ( frame , the coordinate top left , bottom right , color line rectangle , border thickness)\n",
    "                display a circle \n",
    "                cv2.circle(frame , coordinate of center , radius of circle,color ,circle filled or no (with color))\n",
    "                '''\n",
    "                # Draw bounding box \n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "                cv2.circle(img, (center_x, center_y), 5, (0, 255, 0), -1) \n",
    "                ''' \n",
    "                this to display the center x and y (position of the object)\n",
    "                '''\n",
    "                # Display the center position as text on the webcam\n",
    "                center_text = f\"Center: ({center_x}, {center_y})\"\n",
    "                org = (x1, y1 - 10)  \n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                fontScale = 0.7\n",
    "                color = (0, 255, 0)\n",
    "                thickness = 2\n",
    "                ''' \n",
    "                cv2.putText(frame , text ,where to display (position),font, fontScale, color, thickness)\n",
    "                '''\n",
    "                cv2.putText(img, center_text, org, font, fontScale, color, thickness)\n",
    "\n",
    "\n",
    "    # Pass detections to DeepSORT for tracking\n",
    "    # track the object \n",
    "    ''' \n",
    "    Matching Detections to Existing Tracks:\n",
    "        DeepSort attempts to match the new detections (detections) to previously tracked objects using:\n",
    "            Bounding box overlap (Intersection over Union, IoU).\n",
    "            Appearance features (if enabled).\n",
    "        Updating Tracks:\n",
    "            For matched detections, DeepSort updates the state of the corresponding track .\n",
    "        Creating New Tracks:\n",
    "            If a detection cannot be matched to an existing track, DeepSort adds it.\n",
    "        Removing Lost Tracks:\n",
    "            Tracks that have not been updated for multiple frames => delete.\n",
    "        \n",
    "        Return Value:\n",
    "            tracks: A list of track objects\n",
    "            track_id: ID\n",
    "            to_ltrb(): The bounding box coordinates [left, top, right, bottom] of the tracked object.\n",
    "            is_confirmed(): A flag indicating whether the track is active and confirmed.\n",
    "            Optionally, additional information\n",
    "    '''\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "    # Draw a moving dot for each track (phone)\n",
    "    for track in tracks:\n",
    "        \n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "\n",
    "        # Calculate the center of the bounding box for the dot\n",
    "        center_x = int((ltrb[0] + ltrb[2]) // 2)\n",
    "        center_y = int((ltrb[1] + ltrb[3]) // 2)\n",
    "\n",
    "        # Draw the dot at the center of the tracked phone object\n",
    "        cv2.circle(img, (center_x, center_y), 5, (0, 0, 255), -1)  # Red dot for tracking\n",
    "\n",
    "    \n",
    "\n",
    "    # Display the image on webcam with all the added displays \n",
    "    cv2.imshow('Webcam', img)\n",
    "\n",
    "    # press q to quit \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
